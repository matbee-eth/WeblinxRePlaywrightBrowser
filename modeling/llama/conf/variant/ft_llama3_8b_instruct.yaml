# @package _global_
project_name: llama_ft # TODO: Change this to your project name

model:
  # This is meant to be run on 4 gpus with 48GB+ memory (e.g., A6000)
  use_flash_attention_2: True
  name: meta-llama/Meta-Llama-3-8B-Instruct

train:  
  # 4 (# gpus) * 4 (accum steps) * 1 (bsize) = 16 (batch size)
  batch_size_per_device: 4
  gradient_accumulation_steps: 1
  use_accelerator_device_map: True
  use_auto_device_map: False

eval:
  batch_size_per_device: 8
  load_from_save_dir: True