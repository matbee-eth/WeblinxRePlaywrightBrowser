# @package _global_
project_name: llama_ft

model:
  # This is meant to be run on 6 gpus with 48GB+ memory (e.g., A6000)
  use_flash_attention_2: True
  name: meta-llama/Llama-2-13b-chat-hf

train:
  # 6 (# gpus) * 3 (accum steps) * 1 (bsize) = 18 (batch size)
  batch_size_per_device: 1
  gradient_accumulation_steps: 3
  use_accelerator_device_map: True
  use_auto_device_map: False

eval:
  batch_size_per_device: 4
  load_from_save_dir: True